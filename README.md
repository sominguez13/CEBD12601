# CEBD1260
# Final: Correspond to the Final Programming Assignment:

This final assignment have to be complete it using a single jupyter notebook. Furthermore, the answers should include both code and written explanation / interpretation of results.  As described below:

Assignment 1) your first task to is to classify data from a cancer diagnostic database. In this database are patients with tumors, characteristics of those tumors, and biopsy results indicating whether the tumor is Malignant or Benign. In cancer_data.txt you will find the following variables:

   - radius (mean of distances from center to points on the perimeter)
   - texture (standard deviation of gray-scale values)
   - perimeter
   - area
   - smoothness (local variation in radius lengths)
   - compactness (perimeter^2 / area - 1.0)
   - concavity (severity of concave portions of the contour)
   - concave_points (number of concave portions of the contour)
   - symmetry 
   - fractal_dimension ("coastline approximation" - 1)
   - cancer (0 = Benign, 1 = Malignant)  *target*

Use any machine learning algorithm you wish. In your answer include a short description of your  lgorithm of choice and predicted category of a new patient with a tumor with the following features:
   - radius: 14
   - texture: 14
   - perimeter: 88
   - area: 566
   - smoothness: 1
   - compactness: 0.08
   - concavity: 0.06
   - concae points: 0.04
   - symmetry: 0.18
   - fractal dimension: 0.05

Assignment 2) Solving the five 5 bugs (errors), we can answer the following questions:
  1. How many observations are in the training dataset?
  2. How many features are in the training dataset?
  3. How well did your model perform?
  BONUS: Which category is Hockey? 0 or 1? Which category is baseball?
  
  # 1) .ipynb_checkpoints which Correspond to the first assignment named “Assigment1_titanic.csv-checkpoint”. 

In this notebook, we'll train two classifiers to predict survivors in the Titanic dataset. We'll use this classic machine learning problem as a brief introduction to using Apache Spark local mode in a notebook.

# 2) Association Rule Mining Assignment: 
In this assignment we will be identifying frequent item sets from an online retail using the FP-Growth Algorithm in Pyspark. We must download the sample retail data from UCI Machine Learning Repository  and convert the data to .txt after saved as csv. This can be done in by saving as .csv in excel.

# 3) K-means Tutorial Data File:
In this tutorial we created a KNN using numpy as np and from sklearn import neighbors and dataset. The important feels here is classified the data and create predictable models using KNN clusters. 

# 4) Sample_fpgrowth:
Here we have the second assignment which is called “Association Rule Mining with FP-Growth. We used this assignment to identify frequent itemsets using the FP-Growth Algorithm in Pyspark

# 5) Titanic:
In this notebook, we'll train two classifiers to predict survivors in the Titanic dataset. We'll use this classic machine learning problem as a brief introduction to using Apache Spark local mode in a notebook.

# 6) DATA: 
This file contain a file named Mental_Health_in_Tech_company.csv which as downloaded from UCI Machine Learning Repository customized as base. 

